{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Platform Services Registry","text":""},{"location":"index.html#project-overview","title":"Project Overview","text":"<p>\u2003This application facilitates the efficient allocation of OpenShift namespace environments within BC Gov. in response to project team requests. It streamlines the handling of new and update requests, ensuring scalability and flexibility. Key features include a user-friendly request management system, automated provisioning, and comprehensive notification and monitoring capabilities.</p>"},{"location":"index.html#key-technologies-used","title":"Key Technologies Used","text":""},{"location":"index.html#application","title":"Application","text":"<ul> <li> <p>Next.js: Utilized for crafting full-stack web applications, Next.js extends React capabilities and integrates powerful Rust-based JavaScript tools, heightening performance by delivering fully-rendered HTML to the client.</p> </li> <li> <p>Tailwind UI: Harnessed for swift UI development, Tailwind UI's pre-built HTML snippets streamline design workflows by enabling direct style additions to HTML elements.</p> </li> <li> <p>React Hook Form: Simplifies complex form building by reducing code volume and unnecessary re-renders.</p> </li> <li> <p>Zod: A TypeScript-first schema declaration and validation library leveraged for its developer-friendly features and elimination of duplicative type declarations.</p> </li> <li> <p>Mailpit: An email testing tool for developers that acts as an SMTP server, providing a web interface to capture, view, and debug emails.</p> </li> </ul> <p>These technologies foster rapid iterative development, polished user interfaces, and a user-centric experience for application development.</p>"},{"location":"index.html#database-orm","title":"Database &amp; ORM","text":"<ul> <li> <p>MongoDB: Selected for its flexible schema, scalability, high performance, rich querying, automatic failover, and robust community support, making it an ideal choice for diverse applications.</p> </li> <li> <p>Prisma: Utilized for its streamlined development processes, error reduction, and enhanced maintainability, enabling developers to focus more on feature development rather than managing database interactions.</p> </li> </ul>"},{"location":"index.html#run-time-package-version-manager","title":"Run-time Package Version Manager","text":"<ul> <li>asdf: Employed for managing multiple runtime versions, simplifying dependency management, and ensuring consistency across development environments.</li> </ul>"},{"location":"index.html#linters-formatters","title":"Linters &amp; Formatters","text":"<ul> <li> <p>pre-commit: Employed for managing and maintaining multi-language pre-commit hooks to enforce project standards and best practices, reducing the likelihood of bugs or inconsistencies.</p> </li> <li> <p>ESLint: A static code analysis tool ensuring adherence to coding conventions and identifying problematic patterns in JavaScript/Typescript code.</p> </li> <li> <p>ShellCheck: Utilized for static analysis of shell scripts, suggesting improvements and ensuring safer, more efficient, and portable script code.</p> </li> </ul>"},{"location":"index.html#testing-framework","title":"Testing Framework","text":"<ul> <li> <p>Jest: Employed for JavaScript code testing, providing built-in mocking, assertion utilities, and code coverage analysis for efficient and intuitive testing.</p> </li> <li> <p>Cypress: Utilized for end-to-end testing of web applications, offering automatic waiting, real-time reloading, and an interactive test runner for seamless test creation and debugging.</p> </li> </ul>"},{"location":"index.html#deployment-tools","title":"Deployment Tools","text":"<ul> <li> <p>GitHub Actions: A robust CI/CD platform automating various tasks and processes within GitHub repositories, including building, testing, and deploying applications.</p> </li> <li> <p>GitHub Packages: A package hosting service integrated with GitHub repositories, facilitating version control, access control, and dependency management for software packages.</p> </li> <li> <p>Helm: A Kubernetes package manager simplifying deployment, management, and scaling of applications in Kubernetes clusters.</p> </li> <li> <p>release-it: An open-source command-line interface (CLI) tool designed to automate the release process of software projects. It streamlines tasks such as versioning, changelog generation, tagging, and publishing releases to version control systems.</p> </li> </ul>"},{"location":"index.html#configuration-of-code","title":"Configuration of Code","text":"<ul> <li>Terraform: An open-source IaC tool automating provisioning and management of infrastructure resources across various cloud providers, ensuring standardized and efficient deployment with seamless peer review processes and change history tracking within the source control platform.</li> </ul>"},{"location":"index.html#challenges-and-solutions-lessons-learned","title":"Challenges and Solutions / Lessons Learned","text":""},{"location":"index.html#peer-review-optimization","title":"Peer Review Optimization","text":"<p>\u2003A significant challenge the team encountered was optimizing the peer review process to facilitate the effective review and integration of new changes into the main branch with minimal time and effort from colleagues. We identified opportunities for improvement by implementing checks to ensure code quality and seamless integration with the deployment process. Leveraging the <code>pre-commit</code> tool, we conducted code quality checks locally before committing changes, and by extending these checks to our CI pipelines, we reduced the burden on colleagues reviewing linting and formatting issues during peer review. Following the implementation of these automated processes, the peer review workflow became more efficient, and we gained confidence in applying changes to the main branch and deploying them to the development environment. This experience highlighted the importance of continuous integration checks, emphasizing their necessity for ongoing improvement as the project evolves.</p>"},{"location":"index.html#container-image-management-and-deployment","title":"Container Image Management and Deployment","text":"<p>\u2003Navigating container image management and deployment within our continuous deployment process presented challenges, particularly in ensuring efficient building and publishing while maintaining control over image usage. However, this experience highlighted the importance of leveraging available tools effectively to overcome such hurdles. By harnessing <code>GitHub Packages</code> and <code>GitHub Actions</code>, we streamlined our deployment pipelines, enabling seamless building, tagging, and storage of container images. Furthermore, integrating <code>Helm charts</code> facilitated intuitive deployment of updated Kubernetes templates, enhancing overall deployment efficiency. This journey emphasized the value of exploring and leveraging existing tools and features to optimize workflows and address challenges in software development projects.</p>"},{"location":"index.html#ensuring-container-consistency-in-production-deployments-from-testing-environment","title":"Ensuring Container Consistency in Production Deployments from Testing Environment","text":"<p>\u2003Maintaining consistency between the testing and production environments proved challenging. Discrepancies in container images built for production, despite originating from the same codebase as those tested, posed a risk of introducing unexpected issues and compromising production stability. Even with identical codebases, variations during container image generation could lead to subtle differences, potentially disrupting deployments. To address this, we implemented a solution where we reference the container images built for the testing environment in production. This approach minimizes the potential for discrepancies during deployment, ensuring greater reliability and consistency in the production environment.</p>"},{"location":"index.html#automated-change-log-generation","title":"Automated Change Log Generation","text":"<p>\u2003To maintain an accurate record of application changes, we implemented tag-based deployment extensively across upper environments. This approach offers us the flexibility to roll back to previous application versions by updating container image tags and facilitates the generation of change logs based on these tags. Leveraging tools such as <code>conventional-changelog-cli</code> and <code>release-it</code>, we automated this process. Whenever a new tag is generated for testing and production environments after changes are verified, the CHANGELOG.md file is automatically updated by the tool. This automated mechanism ensures that our change log remains up-to-date, reflecting all pertinent modifications made to the application.</p>"},{"location":"index.html#infrastructure-configuration-as-code","title":"Infrastructure Configuration as Code","text":"<p>\u2003Incorporating changes into the infrastructure and other managed services posed a challenge, particularly in integrating the change process into the peer review workflow and maintaining a clear history within the repository. To address this challenge, we implemented <code>Terraform</code> as a solution for managing resources in our infrastructures, including services like Keycloak and Sysdig. This allowed us to define and track changes to infrastructure configurations in code, enabling seamless integration into the peer review process and providing a transparent history of modifications within the repository.</p>"},{"location":"index.html#team-convention-meetings-and-codebase-consistency","title":"Team Convention Meetings and Codebase Consistency","text":"<p>\u2003The absence of team conventions can result in codebase inconsistencies and delays during individual implementation and peer review stages. To address this issue, we organize regular team meetings to establish and document agreed-upon conventions. This approach allows us to discuss common deployment concerns and streamline the deployment process. Following these discussions, we enhance our CI checks where possible and document conventions to ensure they are tracked and adhered to consistently. By implementing this solution, we foster a more cohesive development environment and expedite the deployment process.</p>"},{"location":"index.html#useful-links","title":"Useful Links","text":"<ul> <li>Git Pre-commit Hooks: https://github.com/bcgov/platform-services-registry/blob/main/.pre-commit-config.yaml</li> <li>CI/CD Pipelines: https://github.com/bcgov/platform-services-registry/tree/main/.github/workflows</li> <li>Helm Charts: https://github.com/bcgov/platform-services-registry/tree/main/helm</li> <li>Terraform Scripts: https://github.com/bcgov/platform-services-registry/tree/main/terraform</li> <li>Documentations: https://github.com/bcgov/platform-services-registry/tree/main/docs</li> </ul>"},{"location":"index.html#service-diagrams","title":"Service Diagrams","text":"<pre><code>graph TD;\n    frontendApp--&gt;|HTTP requests| backendAPI;\n    backendAPI--&gt;|Queries| database;\n    backendAPI--&gt;|Sends emails| emailService(CHES);\n    backendAPI--&gt;|Provisions namespaces| namespaceProvisioner(NATS);\n    backendAPI--&gt;|Queries| MSGraphAPI(MS graph API);\n    backendAPI--&gt;|Handles authentication &amp; authorization| authAuthorization(Keycloak);\n    backendAPI--&gt;|Manages publice cloud users| publicCloudUserManagement(Keycloak);\n    backendAPI--&gt;|Interacts with| openshiftAPI(Openshift API);\n</code></pre>"},{"location":"api-testing.html","title":"API Testing","text":"<p>We use <code>Jest</code> to test our application's API endpoints, focusing on access permissions and parameter validations.</p>"},{"location":"api-testing.html#steps-to-add-api-test-scripts","title":"Steps to Add API Test Scripts","text":"<ol> <li>Add the corresponding API service methods to encapsulate endpoint information and structure in the <code>/app/services/api-test</code> directory.</li> <li>The names and method arguments should ideally be similar to those in <code>/app/services/backend</code>.</li> <li>Add the test script, such as <code>route.test.ts</code>, in the same directory as the API route file.</li> <li>The test script should focus on the permission checks and parameter validations of the route itself. When using other endpoint methods from <code>/app/services/api-test</code>, only check the status codes to prevent the test script from becoming overly bloated with excessive validations; concentrate on the route tests for each test script.</li> </ol>"},{"location":"bot-pr-review.html","title":"Renovate Bot PR Review","text":"<p>Renovate Bot is our tool for automatically updating dependencies. However, a manual PR review process is necessary when the bot creates PRs for dependency updates.</p>"},{"location":"bot-pr-review.html#review-criteria","title":"Review Criteria","text":"<p>When reviewing a Renovate Bot PR, consider the following to assess whether it's safe to merge into the main branch:</p> <ol> <li> <p>Area of Usage    Identify where the tool or package is used:</p> </li> <li> <p>Application production</p> </li> <li>Application development</li> <li>Sandbox services</li> <li>Terraform</li> <li>Helm charts</li> <li>Continuous integration/Code quality</li> <li> <p>Low-level tools</p> </li> <li> <p>Functionality    Understand what the tool or package does within the identified area.</p> </li> <li> <p>Upgrade Level    Determine the upgrade type: major, minor, or patch.</p> </li> <li> <p>Pipeline Checks    Review if any checks are related to the changes and ensure they pass.</p> </li> </ol> <p>Based on these criteria, assess the risk to decide whether to merge the PR.</p>"},{"location":"bot-pr-review.html#handling-failed-checks","title":"Handling Failed Checks","text":"<p>If a pipeline check fails after an upgrade, it indicates a potential breaking change. Investigate the specific failed check and review the detailed error message to identify the root cause. If a codebase change is required, check out the PR branch locally to debug and make the necessary updates directly on the PR branch.</p>"},{"location":"ci-cd.html","title":"CI/CD Pipelines","text":""},{"location":"ci-cd.html#release-tag-dispatch-workflow","title":"Release Tag Dispatch Workflow","text":"<p>When creating a new tag using the repository's default <code>GITHUB_TOKEN</code> to perform tasks on behalf of the GitHub Actions, events triggered by the <code>GITHUB_TOKEN</code> will not create a new workflow run. To address this behavior, we need to set up an <code>SSH Key</code> when fetching the repository in the pipeline.</p> <ol> <li>Generate a new OpenSSH Key:</li> </ol> <pre><code>ssh-keygen -t ed25519 -f id_ed25519 -N \"\" -q -C \"\"\n</code></pre> <p>This command will generate a private key <code>id_ed25519</code> and a corresponding public key <code>id_ed25519.pub</code> in the working directory.</p> <ol> <li>Add the private key to GitHub's <code>Secrets</code>, naming it <code>SSH_KEY</code> in the repository.</li> <li>Add the public key to GitHub's <code>Deploy keys</code>, also naming it <code>SSH_KEY</code> in the repository.</li> <li>In the GitHub Actions, specify the <code>ssh-key</code> option:</li> </ol> <pre><code>jobs:\n  tag-changelog:\n    runs-on: ubuntu-22.04\n    steps:\n    - uses: hmarr/debug-action@v2\n    - uses: actions/checkout@v4\n      with:\n        ssh-key: ${{ secrets.SSH_KEY }}\n        fetch-depth: 0\n</code></pre>"},{"location":"database.html","title":"Database","text":""},{"location":"database.html#backup-and-restore-procedures","title":"Backup and Restore Procedures","text":"<p>To facilitate the backup and restore processes, we utilize a container running MongoDB tools available at database-tools.</p>"},{"location":"database.html#backup-configuration","title":"Backup Configuration","text":"<ul> <li>The Kubernetes deployment template for the backup process can be found at mongodb-backup.yaml. This template orchestrates the deployment of the MongoDB tools container.</li> <li>A notification mechanism has been set up to inform the designated RocketChat channel about the status of the backup operation.</li> </ul>"},{"location":"database.html#backup-steps","title":"Backup Steps","text":"<p>The backup procedure is initiated at regular intervals according to the cron schedule set by the backup container. However, there are occasions when manual backup is required, such as before deploying to the production environment. To back up the database, follow these steps:</p> <pre><code>oc rsh &lt;backup-pod-name&gt;\nmongo-archive --db=pltsvc --read-preference=secondaryPreferred --force-table-scan --cron=false\n</code></pre> <p>Ensure that the new backup file is generated in the designated backup directory <code>/home/nonroot/backup</code>.</p>"},{"location":"database.html#restore-steps","title":"Restore Steps","text":"<p>To restore the database, follow these steps:</p> <pre><code>oc rsh &lt;backup-pod-name&gt;\nmongo-unarchive --uri=\"mongodb://&lt;primary-pod-name&gt;.pltsvc-mongodb-headless/?authSource=admin\" --db=pltsvc\n</code></pre> <p>Execute the provided shell commands to access the backup pod and initiate the restoration process. The mongo-unarchive command uses the specified MongoDB URI, which includes the <code>primary pod name</code> and authentication details from the admin source. The restoration is targeted for the <code>pltsvc</code> database.</p>"},{"location":"debug-deployments.html","title":"Debugging Helm Deployment Failure Issues","text":""},{"location":"debug-deployments.html#error-upgrade-failed-another-operation-installupgraderollback-is-in-progress","title":"Error: UPGRADE FAILED: another operation (install/upgrade/rollback) is in progress","text":"<p>This error usually occurs when multiple Helm operations\u2014install, upgrade, or rollback\u2014are triggered in quick succession. Here\u2019s how to resolve it:</p>"},{"location":"debug-deployments.html#using-the-ocp-ui","title":"Using the OCP UI","text":"<ol> <li>Log in to the OCP namespace.</li> <li>From the left sidebar, select Helm.</li> <li>Go to the Helm Releases tab and find the target release.</li> <li>Open the Revision History tab.</li> <li>Locate the most recent successful release (Status: Deployed).</li> <li>Click the three dots on the right and select Rollback.</li> <li>Wait for the rollback to finish.</li> <li>Rerun the failed GitHub Action deployment pipeline.</li> </ol>"},{"location":"debug-deployments.html#using-the-cli","title":"Using the CLI","text":"<ol> <li>Check the current status of the release:</li> </ol> <pre><code>helm status pltsvc\n</code></pre> <ol> <li>List all revisions for the <code>pltsvc</code> release:</li> </ol> <pre><code>helm history pltsvc\n</code></pre> <p>This command displays the release history, showing revision numbers and statuses such as <code>DEPLOYED</code>, <code>FAILED</code>, or <code>SUPERSEDED</code>.</p> <ol> <li> <p>Identify the last successful release:    Look for the latest revision with a <code>DEPLOYED</code> status.</p> </li> <li> <p>Rollback to the desired revision:</p> </li> </ol> <pre><code>helm rollback pltsvc &lt;revision_number&gt;\n</code></pre> <p>Replace <code>&lt;revision_number&gt;</code> with the number of the last successful revision.</p> <ol> <li>Verify the rollback by checking the release status:    <pre><code>helm status pltsvc\n</code></pre></li> </ol>"},{"location":"developer-guide.html","title":"Developer Guidelines","text":""},{"location":"developer-guide.html#setting-up-wsl-windows-users-only","title":"Setting Up WSL (Windows users only)","text":"<p>WSL allows you to effortlessly incorporate a Linux environment into your Windows operating system. This method empowers developers to leverage the robust development tools of Linux while remaining within their Windows ecosystem. For the installation instructions of WSL, please refer to the following links:</p> <ul> <li>How to install Linux on Windows with WSL</li> <li>Manual installation steps for older versions of WSL.</li> </ul>"},{"location":"developer-guide.html#setting-up-github-project-repository","title":"Setting Up GitHub project repository","text":""},{"location":"developer-guide.html#connecting-to-github-with-ssh","title":"Connecting to GitHub with SSH","text":"<p>For an enhanced method of authentication when interacting with GitHub repositories, employing an SSH key is highly advisable, as opposed to the less secure username and password authentication. For detailed instructions, refer to the GitHub documentation:</p> <ul> <li>Generating a New SSH Key and Adding it to the SSH Agent.</li> </ul>"},{"location":"developer-guide.html#cloning-repository","title":"Cloning Repository","text":"<p>To clone a repository using SSH and set up essential Git configurations, you can execute the following shell commands:</p> <pre><code># Clone the repository via SSH\ngit clone git@github.com:bcgov/platform-services-registry.git\n\n# Change into the cloned repository directory\ncd platform-services-registry\n</code></pre>"},{"location":"developer-guide.html#gpg-key-signing","title":"GPG key signing","text":"<p>To ensure the legitimacy of Git commits, it is strongly recommended to sign them using a GPG key. For step-by-step guidance, please consult the GitHub documentation:</p> <ul> <li>Signing Commits</li> <li>Adding a New GPG Key to Your GitHub Account</li> </ul> <p>To enable GPG signing in Git, follow these steps in the repository:</p> <pre><code># Define the signing key hash\ngit config user.signingkey \"&lt;hash&gt;\"\n\n# Specify Git to sign commits and tags with GPG\ngit config commit.gpgsign true\ngit config tag.gpgsign true\n\n# Set the GPG program for signing (if not already set)\ngit config gpg.program gpg\n</code></pre>"},{"location":"developer-guide.html#setting-up-the-local-development-environment","title":"Setting up the local development environment","text":"<ul> <li>Using Linux or MacOS terminals is advised for the development of web applications and managing their pipelines.</li> <li><code>asdf</code> is a tool to manage the required packages with specific versions.</li> <li>All the packages are defined in <code>tool-versions</code>.</li> </ul>"},{"location":"developer-guide.html#installation","title":"Installation","text":"<ol> <li>Install <code>asdf</code> according to the <code>asdf</code> installation guide.</li> <li>https://asdf-vm.com/guide/getting-started.html#getting-started</li> <li>Install <code>asdf</code> packages defined in <code>.tool-versions</code>.    <pre><code>cat .tool-versions | cut -f 1 -d ' ' | xargs -n 1 asdf plugin-add || true\nasdf plugin-add docker-compose https://github.com/virtualstaticvoid/asdf-docker-compose.git || true\nasdf plugin-update --all\nasdf install\nasdf reshim\n</code></pre></li> <li>Confirm the packages installed.    <pre><code>asdf current\n</code></pre></li> <li>Install python packages.    <pre><code>pip install -r requirements.txt\n</code></pre></li> <li>Install the pre-commit script.    <pre><code>pre-commit install\npre-commit install --hook-type commit-msg\n</code></pre></li> </ol>"},{"location":"developer-guide.html#setting-up-github-workspace","title":"Setting Up GitHub Workspace","text":""},{"location":"developer-guide.html#working-on-features","title":"Working on features","text":"<ol> <li>Create a <code>feature</code> branch.    <pre><code>git checkout -b \"feat/1091\"\n</code></pre></li> <li>Make sure the branch is rebased onto <code>main</code> branch.    <pre><code>git pull origin main --rebase\n</code></pre></li> <li>Make changes to complete the task.</li> <li>Make a commit with the changes.    <pre><code>git add .\ngit commit -m \"feat(1091): add new page\" # `pre-commit` hooks will be triggered to ensure the code quality.\ngit pull origin main --rebase\n</code></pre></li> <li>Push the commit to the remote repository.    <pre><code>git push\n</code></pre></li> <li>Make a PR from the feature branch into the target branch via UI.</li> <li>Wait until the checks pass before requesting the peer review via UI.</li> <li>Once the PR is approved, merge the PR via UI.</li> </ol>"},{"location":"email-scenarios-private-cloud.html","title":"Email scenarios private cloud","text":""},{"location":"email-scenarios-private-cloud.html#email-scenarios","title":"Email Scenarios","text":""},{"location":"email-scenarios-private-cloud.html#private-cloud","title":"Private Cloud","text":""},{"location":"email-scenarios-private-cloud.html#scenario-1-product-create-request","title":"Scenario 1. Product Create Request","text":"<p>Description: A new or existing user submits a request via the registry to have a namespace for their product. This is assuming the user has already had an onboarding meeting, and meets the requirements for submitting a create request. Upon submitting the create request, the following emails may trigger.</p> <ol> <li> <p>Notification sent to admins containing:   <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s), Cluster)</li> <li>Actioned by text (who submitted the request)</li> </ol></p> </li> <li> <p>Notification sent to Product PO/TL(s) containing:     <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s), Cluster)</li> </ol></p> </li> <li> <p>Decision: Create request Approval/Rejection by Admin</p> </li> <li> <p>3a. Approval sent to PO/TLs containing:   <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s))</li> <li>Namespace Details (Cluster, Link to all four namespaces, Default values of namespaces)</li> <li>Security Tools Info</li> <li>Artifactory Info</li> <li>Vault Info</li> <li>ACS Info</li> <li>Sysdig Info</li> </ol> </p> </li> <li> <p>3b. Rejection sent to PO/TLs containing:   <ol> <li>Product Details (Name)</li> <li>Admin review comments</li> </ol> </p> </li> </ol> <pre><code>flowchart LR\n    A((User Submits &lt;br&gt; Create Request)) --&gt; B(PrivateCloudAdminCreateRequest&lt;br&gt;PrivateCloudCreateRequest)\n    B --&gt; C{Admin decides on the create request}\n    C --&gt;|Create Request Approved and Provisioned| D(PrivateCloudRequestProvisioned)\n    C --&gt;|Create Request Rejected| E(PrivateCloudCreateRejected)\n</code></pre>"},{"location":"email-scenarios-private-cloud.html#scenario-2-edit-request-resource-quota-increase","title":"Scenario 2. Edit Request: Resource Quota Increase","text":"<p>Description: A PO/TLs of a product submits a request for more CPU, memory, and/or storage of 1 or more namepspaces. Upon submitting the edit resource request, the following emails will trigger.</p> <ol> <li> <p>Notification sent to admins containing:   <ol> <li>Description Changes (Product Name, Description, Ministry)</li> <li>Contact Changes (Product Owner, Primary Technical Lead, Secondary Technical Lead)</li> <li>Comments by user if provided</li> <li>Resource Quotas with Current and Requested</li> <li>Actioned by text (who submitted the request)</li> </ol></p> </li> <li> <p>Notification sent to PO/TL(s) containing:   <ol> <li>Description Changes (Product Name, Description, Ministry)</li> <li>Contact Changes (Product Owner, Primary Technical Lead, Secondary Technical Lead)</li> <li>Comments by user if provided</li> <li>Resource Quotas with Current and Requested</li> </ol></p> </li> <li> <p>Decision: Resource Quota Edit Request Approval/Rejection by admins</p> </li> <li> <p>3a. Approval sent to PO/TLs containing:   <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s))</li> <li>Namespace Details with Previous and Approved values (Cluster, Link to all four namespaces)</li> </ol> </p> </li> <li> <p>3b. Rejection Sub-Scenario containing:   <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s))</li> <li>Admin review comments</li> <li>Namespace Details with Previous and Rejected values (Cluster, Link to all four namespaces)</li> </ol> </p> </li> </ol> <pre><code>flowchart LR\n    A((User submits a &lt;br&gt;Edit Resource &lt;br&gt; Quota Request)) --&gt; B(PrivateCloudResourceRequestAdmin&lt;br&gt;PrivateCloudResourceRequest)\n    B --&gt; C{Admin decision on Edit Resource Quota request}\n    C --&gt;|Request approved| D(PrivateCloudResourceRequestApproval)\n    C --&gt;|Request rejected| E(PrivateCloudResourceRequestRejection)\n    D --&gt;|Changes provisioned| F(PrivateCloudResourceRequestCompleted)\n</code></pre>"},{"location":"email-scenarios-private-cloud.html#scenario-3-edit-request-no-resource-quota-increase","title":"Scenario 3. Edit Request: No Resource Quota Increase","text":"<p>Description: When a PO/TLs of a product edit their product where there is no increase in resources requested, the following emails will trigger.</p> <ol> <li>Summary of the changes provisioned sent to PO/TLs containing:   <ol> <li>Updated Description Changes (Product Name, Description, Ministry)</li> <li>Updated Contact Changes</li> <li>Updated Resource Quota Downgrades</li> <li>Description Changes (Product Name, Description, Ministry)</li> <li>Contact Changes (Product Owner, Primary Technical Lead, Secondary Technical Lead)</li> <li>Comments by user if provided</li> <li>Resource Quotas with Current and Requested</li> </ol> </li> </ol> <pre><code>flowchart LR\n    A((User Submits Edit w/ &lt;br&gt; No Resource Increase)) --&gt;|Request Automatically Approved and Provisioned| C(PrivateCloudEditApprovedAndProvisioned)\n</code></pre>"},{"location":"email-scenarios-private-cloud.html#scenario-4-mixed-edit-request-quota-increase-and-contact-change","title":"Scenario 4. Mixed Edit Request: Quota Increase and Contact Change","text":"<p>Description: When a PO/TLs of a product edit their product with both Quota Increase and Contact Change.</p> <ol> <li> <p>A notification sent to admins containing:     <ol> <li>Comments by user if provided</li> <li>Name, Description, Ministry, Cluster, Contacts (all can be with Current and Requested sections if they are changed)</li> <li>Quota Changes with Current and Requested sections</li> </ol></p> </li> <li> <p>Summary of changes submitted sent to PO/TLs containing:   <ol> <li>Product Details that were changed, Current and Requested (Name, Description, Ministry, Contacts of PO/TL(s), Quota)</li> </ol> </p> </li> <li> <p>Decision: Mixed Quota Increase and Contact Change Request Approval/Rejection by admins</p> </li> <li> <p>3a. Approval sent to PO/TLs containing:   <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s))</li> <li>Namespace Details with Previous and Approved values (with link to the namespace that changes were made on)</li> </ol> </p> </li> <li> <p>3b. Rejection Sub-Scenario containing:   <ol> <li>Product Details that were changed with Current and Rejected Quota</li> <li>Admin review comments</li> </ol> </p> </li> </ol> <pre><code>flowchart LR\n    A((User submits a&lt;br&gt; Mixed Edit Request &lt;br&gt;Quota and Contacts)) --&gt; B(PrivateCloudResourceRequestAdmin&lt;br&gt;PrivateCloudResourceRequest)\n    B --&gt; C{Admin decision on Mixed Edit Request}\n    C --&gt;|Request approved| D(PrivateCloudResourceRequestApproval)\n    C --&gt;|Request rejected| E(PrivateCloudResourceRequestRejection)\n    D --&gt;|Changes provisioned| F(PrivateCloudResourceRequestCompleted)\n</code></pre>"},{"location":"email-scenarios-private-cloud.html#scenario-5-delete-request-is-submitted","title":"Scenario 5. Delete request is submitted","text":"<p>Description: When a PO/TLs of a product have removed all PVC's and resources deployed on their namespaces of a given product, then a user is eligible to submit a delete request. Upon a successful deletion check the user can submit the delete request and thus the following emails may trigger.</p> <ol> <li> <p>A notification sent to admins containing:   <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s))</li> <li>Actioned by text (who submitted the request)</li> </ol></p> </li> <li> <p>A summary sent to PO/TLs containing:   <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s))</li> <li>Namespace Details (Cluster and Link to the 4 namespaces)<li> </li> <li> <p>Decision: Product Deletion Approval/Rejecton by admins</p> </li> <li> <p>3a. Approval confirmation sent to PO/TLs containing:     <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s))</li> <li>Namespace Details (Cluster and Link to the 4 namespaces)</li> </ol></p> </li> <li>3a. Rejection sent to PO/TLs containing:     <ol> <li>Review comments</li> </ol></li> <pre><code>flowchart LR\n    A((User submits a &lt;br&gt; delete request)) --&gt; B(PrivateCloudAdminDeleteRequest&lt;br&gt;PrivateCloudDeleteRequest)\n    B --&gt; C{Admin decides on the request}\n    C --&gt;|Request approved| D(PrivateCloudDeleteApproved)\n    C --&gt;|Request rejected| E(PrivateCloudDeleteRejected)\n</code></pre>"},{"location":"email-scenarios-public-cloud.html","title":"Email scenarios public cloud","text":""},{"location":"email-scenarios-public-cloud.html#email-scenarios","title":"Email Scenarios","text":""},{"location":"email-scenarios-public-cloud.html#public-cloud","title":"Public Cloud","text":""},{"location":"email-scenarios-public-cloud.html#scenario-1-product-create-request","title":"Scenario 1. Product Create Request","text":"<p>Description: A new or existing user submits a request via the registry to have a product on the Public Cloud Landing Zone. This is assuming the user has already had an onboarding meeting, and meets the requirements for submitting a create request. Upon submitting the create request, the following emails may trigger.</p> <ol> <li> <p>Notification sent to admins containing:   <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s)/EA)</li> <li>Landing Zone Details (Provider, Budget details, Account Coding)</li> <li>First/Last name of a user who actioned the request</li> </ol></p> </li> <li> <p>Notification sent to Product PO/TL(s) containing:     <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s)/EA)</li> <li>Landing Zone Details (Provider, Budget details, Account Coding)</li> </ol></p> </li> <li> <p>Decision: Create request Approval/Rejection by Admin</p> </li> <li> <p>3a.I. Approval sent to PO/TLs containing:   <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s)/EA)</li> <li>Landing Zone Details (Provider, Budget details, Account Coding)</li> </ol></p> </li> <li> <p>3a.II. Notification sent to Expense Authority containing:   <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s)/EA, Licence Plate)</li> </ol></p> </li> <li> <p>3b. Rejection sent to PO/TLs containing:   <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s)/EA)</li> <li>Admin review comments</li> </ol> </p> </li> </ol> <pre><code> flowchart LR\n    A((User Submits a &lt;br&gt; Create Request)) --&gt; B(PublicCloudAdminCreateRequest&lt;br&gt;PublicCloudCreateRequest)\n    B --&gt; C{Admin decides on the create request}\n    C --&gt;|Create Request Approved and Provisioned| D(PublicCloudRequestProvisioned&lt;br&gt;ExpenseAuthorityNotification)\n    C --&gt;|Create Request Rejected| E(PublicCloudCreateRejected)\n</code></pre>"},{"location":"email-scenarios-public-cloud.html#scenario-2-edit-request","title":"Scenario 2. Edit Request","text":"<p>Description: A PO/TLs of a product submits a request changing some details of the existing product. Since there is no decision process for editing a product, a summary of the previous and updated changes are triggered.</p> <ol> <li> <p>Summary sent to PO/TLs containing:   <ol> <li>User Comments</li> <li>Description Changes (Name, Description, Ministry)</li> <li>Contact Changes</li> <li>Budget Changes</li> <li>Account Coding Changes</li> </ol></p> </li> <li> <p>If EX changed summary sent to EA containing:</p> </li> </ol> <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s)/EA, Licence Plate)</li> </ol> <pre><code>   flowchart LR\n     A((User Submits an &lt;br&gt; Edit Request)) --&gt; |Request Provisioned|B(Edit Request Received sent to PO/TLs)\n   B(PublicCloudEditSummary) --&gt; C{EX changed}\n    C --&gt;|EX was changed| D(ExpenseAuthorityNotification)\n</code></pre>"},{"location":"email-scenarios-public-cloud.html#scenario-3-delete-request-is-submitted","title":"Scenario 3. Delete request is submitted","text":"<p>Description: When a PO/TLs of a product are ready to decomission their product off the Public Cloud Landing Zone the user can submit a delete request and thus the following emails may trigger.</p> <ol> <li> <p>A notification sent to admins containing:   <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s))</li> <li>Landing Zone Details (Provider, Budget details, Account Coding)</li> <li>First/Last name of a user who actioned the request</li> </ol></p> </li> <li> <p>A summary sent to PO/TLs containing:   <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s))</li> <li>Landing Zone Details (Provider, Budget details, Account Coding)</li> </ol></p> </li> <li> <p>Decision: Product Deletion Approval/Rejecton by admins</p> </li> <li> <p>3a. Approval confirmation sent to PO/TLs containing:     <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s)/EA)</li> <li>Landing Zone Details (Provider, Budget details, Account Coding)</li> </ol></p> </li> <li>3a. Rejection sent to PO/TLs containing:     <ol> <li>Product Details (Name, Description, Ministry, Contacts of PO/TL(s)/EA)</li> <li>Review comments</li> </ol></li> </ol> <pre><code>flowchart LR\n    A((User submits a &lt;br&gt; delete request)) --&gt; B(PublicCloudAdminDeleteRequest&lt;br&gt;PublicCloudDeleteRequest)\n    B --&gt; C{Admin decides on the request}\n    C --&gt;|Request approved| D(PublicCloudDeleteApproved)\n    C --&gt;|Request rejected| E(PublicCloudDeleteRejected)\n</code></pre>"},{"location":"emou-workflow.html","title":"Emou workflow","text":""},{"location":"emou-workflow.html#public-cloud-emou-workflow-diagram","title":"Public Cloud eMOU Workflow Diagram","text":"<pre><code>flowchart TB\n    A((#quot;Product Team#quot; creates&lt;br&gt;a new product&lt;br&gt;with billing details))\n    A --&gt; B{System checks&lt;br&gt;if the account coding&lt;br&gt;already exists}\n    B --&gt;|If exists| B1A(Checkbox displayed:&lt;br&gt;'Our records show that your team already has a signed MoU with OCIO for AWS/Azure use.&lt;br&gt;This new product will be added to the existing MoU.&lt;br&gt;A copy of the signed MoU for this product will be emailed to the Ministry Expense Authority.')\n    B1A --&gt; B1B(Submit form)\n    B1B --&gt; C\n    B --&gt;|If not exists| B2A(Submit form)\n    B2A --&gt; B2B(Send eMOU signing request&lt;br&gt;email to #quot;Expense Authority#quot;)\n    B2B --&gt; B2C(#quot;Expense Authority#quot; logs in&lt;br&gt;and signs the eMOU)\n    B2C --&gt; B2D(Send eMOU review request&lt;br&gt;email to #quot;Cloud Expense Authority Director#quot;)\n    B2D --&gt; B2E(#quot;Cloud Expense Authority Director#quot; logs in&lt;br&gt;and approves the eMOU)\n    B2E --&gt; C(Send new product review request email to #quot;Public Cloud Admin#quot;)\n    C --&gt; D(#quot;Public Cloud Admin#quot; logs in&lt;br&gt;and approves the new product request)\n    D --&gt; E[Workspace is provisioned&lt;br&gt;with billing information non-editable]\n</code></pre>"},{"location":"git-tips.html","title":"Git Tips","text":""},{"location":"git-tips.html#how-to-create-a-clean-pr","title":"How to create a clean PR","text":"<p>In certain scenarios, it becomes necessary to refine commit history and generate a pristine pull request for the target branch. While Git rebase is a useful tool for this purpose, its application can be less straightforward, especially when the working branch incorporates changes from other pull requests.</p> <p>To address this, a practical workaround involves resetting all commits on the base branch. Here's a simple guide to follow:</p> <ol> <li>Confirm that you are on the working branch with a cluttered commit history.</li> <li>Create a new branch:</li> </ol> <pre><code>git checkout -b &lt;new-branch&gt;\n</code></pre> <ol> <li>Pull Changes from the Base Branch:</li> </ol> <pre><code>git pull origin &lt;base-branch&gt; --no-rebase\n</code></pre> <ul> <li> <p>Resolve any merge conflicts and commit the changes.</p> </li> <li> <p>Reset all commits based on the base branch (usually the default repository branch):    <pre><code>git reset --soft origin/&lt;base-branch&gt;\n</code></pre></p> </li> <li> <p>For instance:</p> </li> </ul> <pre><code>git checkout -b feat/1234-1\ngit pull origin main --no-rebase\ngit add .\ngit commit -m \"chore(0000): temporary message\"\ngit reset --soft origin/main\ngit add .\ngit commit -m \"feat(0000): &lt;commit-message&gt;\"\n</code></pre>"},{"location":"onboarding-offboarding.html","title":"Onboarding and Offboarding Documentation","text":"<p>This document outlines the necessary steps for onboarding and offboarding developers for our application. It includes role assignments, access requirements, and communication channels to ensure a smooth transition.</p>"},{"location":"onboarding-offboarding.html#1-role-assignments","title":"1. Role Assignments","text":"<p>-Live OCP consoles</p> <p>-Dev, Test, Prod Live Environments</p> <p>-Dev, Test, Prod Live Keycloak realms</p>"},{"location":"onboarding-offboarding.html#2-access-requirements","title":"2. Access Requirements","text":"<p>-GitHub Repository Access</p> <p>-Zenhub Board</p> <p>-Uptime Access</p> <p>-Platform Services Google Drive Access</p>"},{"location":"onboarding-offboarding.html#3-communication-channels","title":"3. Communication Channels","text":""},{"location":"onboarding-offboarding.html#internal-rocketchat-channels","title":"Internal Rocket.Chat Channels","text":"<ul> <li>#app-dev-team</li> <li>#app-dev-internal:</li> <li>#internal-platform-services</li> <li>#internal-devops-registry</li> <li>#internal-provisioner</li> </ul>"},{"location":"onboarding-offboarding.html#external-rocketchat-channels","title":"External Rocket.Chat Channels","text":"<ul> <li>#devops-operations</li> <li>#devops-how-to</li> <li>#devops-alerts</li> <li>#devops-sos</li> <li>#devops-registry</li> <li>#labops-alerts</li> <li>#labops-requests</li> <li>#rocky-chat</li> <li>#random</li> <li>#general</li> <li>#rocketchat-help</li> <li>#kudos</li> <li>#github-news-and-support</li> </ul>"},{"location":"onboarding-offboarding.html#microsoft-teams-channels","title":"Microsoft Teams Channels","text":"<ul> <li>External: DOCS Branch</li> <li>External: Digital Office</li> </ul>"},{"location":"onboarding-offboarding.html#4-offboarding-process","title":"4. Offboarding Process","text":""},{"location":"onboarding-offboarding.html#revoking-access","title":"Revoking Access","text":"<ol> <li>Live OCP consoles:</li> <li>Remove the developer's access to the openshift console namespaces.</li> <li>GitHub Repository:</li> <li>Remove the developer\u2019s GitHub username from the repo admin list.</li> <li>Uptime Monitoring:</li> <li>Revoke login credentials.</li> <li>Google Drive:</li> <li>Remove access to shared folders.</li> </ol>"},{"location":"onboarding-offboarding.html#updating-roles","title":"Updating Roles","text":"<ul> <li>Keycloak:</li> <li>Remove roles for Dev, Test, and Prod environments.</li> <li>Remove realm roles for Dev, Test, and Prod.</li> </ul>"},{"location":"onboarding-offboarding.html#communication-channels","title":"Communication Channels","text":"<ul> <li>Rocket.Chat:</li> <li>Remove the developer from all internal channels.</li> <li>Teams:</li> <li>Remove from External DOCS Branch and Digital Office channels.</li> </ul>"},{"location":"react-email.html","title":"React email","text":""},{"location":"react-email.html#react-emails","title":"React Emails","text":"<p>react-emails is a package that allows for the creation of emails using React and TypeScript. For more info checkout the following links:</p> <ol> <li>Website: https://react.email/</li> <li>Github: https://github.com/resendlabs/react-email</li> </ol>"},{"location":"react-email.html#development-email-templates-server","title":"Development Email-Templates Server","text":"<p>This package comes with a CLI that creates a dev server for you to view the emails.</p> <ol> <li>Run the react-email development server that will run on <code>localhost:3001</code></li> </ol> <pre><code>npm run email\n</code></pre> <ul> <li> <p>If you want to view images, you will also have to run the normal development server with <code>npm run dev</code></p> </li> <li> <p>Go to http://localhost:3001/ to view the email dashboard</p> </li> <li> <p>react-email will only display the .tsx files in the base <code>emails</code> folder</p> </li> <li>The <code>templates</code> folder contains the templates that are called by the emailHandler</li> <li><code>/components/params.tsx</code> contains the mock data to view the email templates</li> </ul>"},{"location":"react-email.html#react-email-commands","title":"React-Email Commands","text":"<p>react-email comes with a <code>dev</code>, <code>build</code>, <code>start</code> and <code>export</code> command. The way these commands can be called are found within the <code>package.json</code> script.</p> <p>The <code>dev</code>, <code>build</code> and <code>start</code> commands work like your normal npm commands, with the difference being that project it generates is a dashboard which shows your email templates. They create a folder called .react-email which can essentially be treated as another project folder. It is important to note that running these commands in the will overwrite some of the folders in .react-email, such as the package.json file.</p> <p><code>export</code> creates a folder called email-export containing the html renders for all the existing templates. It works similarly to how the render() function works.</p>"},{"location":"react-email.html#deploying-the-email-dashboard","title":"Deploying the email Dashboard","text":"<p>The problem when trying to deploy the dashboard was that <code>npm run build</code> was failing due to linting and typescript errors that exists in how <code>.react-email</code> imports the email templates</p> <pre><code>import Mail from '../../emails/PrivateCloudAdminCreateRequest.tsx';`\n</code></pre> <p>We can always update the config manually to ignore the above error, but the initial creation of the .react-email folder (if it does not already exist) will not include those changes. For that reason, our docker image will run <code>build</code> command twice. Once to generate the .react-email folder for our image, and once more after we update the next.config.js file with the following rule.</p> <pre><code>   typescript: {\n      ignoreBuildErrors: true,\n   },\n</code></pre> <p>The Dockerfile for the email-dashboard will then be pretty much the same as a normal deployment, with the exception of the context for <code>build</code> and <code>start</code></p>"},{"location":"resource-metrics.html","title":"Resource metrics","text":""},{"location":"resource-metrics.html#resource-metrics-setup","title":"Resource Metrics Setup","text":"<p>For Registry to get metrics from Prometheus, the following things are needed:</p> <ul> <li>a Service Account <code>registry-metrics-reader</code></li> <li>the cluster role and rolebinding for the SA to access metrics from all namespaces</li> <li>a non-rotating service account token secret <code>registry-metrics-reader-sa-token</code> that can be used for Prometheus query</li> </ul> <p>These are created as part of the CCM from https://github.com/bcgov-c/platform-gitops-gen/tree/master/roles/cluster_checks/files</p> <p>As a <code>admin</code> of the cluster, to obtain the SA token for the query and test it out:</p> <pre><code># log to the cluster:\noc login --token=&lt;token&gt; --server=https://api.&lt;cluster&gt;.devops.gov.bc.ca:6443\n# get the token:\nexport REGISTRY_TOKEN=$(oc get secret registry-metrics-reader-sa-token -n gitops-tools -o jsonpath='{.data.token}' | base64 --decode)\n# check token:\necho $REGISTRY_TOKEN\n# test query (this is just an example, make sure to update the URL and query used for different environments)\ncurl \\\n-H 'Accept: application/json' \\\n-H \"Authorization: Bearer $REGISTRY_TOKEN\" \\\n-d 'query=sum(pod:container_cpu_usage:sum{namespace=\"e9b123-prod\", pod=~\"getok-app.*\"})' \\\nhttps://thanos-querier-openshift-monitoring.apps.klab.devops.gov.bc.ca/api/v1/query\n</code></pre>"},{"location":"rocketchat-notifications.html","title":"RocketChat Deployment Notifications Setup","text":"<p>To facilitate efficient communication of deployment actions, we have set up a notification bot to notify developers and stakeholders about the status of our deployments, as well as provide a convenient link to the changes made.</p>"},{"location":"rocketchat-notifications.html#prerequisites","title":"Prerequisites","text":"<ul> <li>You must be an admin of this repository.</li> <li>You must be a member of the <code>#app-dev-team</code> RocketChat channel.</li> </ul>"},{"location":"rocketchat-notifications.html#setting-up-and-updating-the-notification-bot","title":"Setting Up and Updating the Notification Bot","text":""},{"location":"rocketchat-notifications.html#1-create-a-rocketchat-webhook","title":"1. Create a RocketChat Webhook","text":"<p>To create a webhook in RocketChat for deployment notifications:</p> <ol> <li>Open RocketChat and click on the vertical 3-dot menu at the top-left of the application.</li> <li>Select Workspace under the Administration heading.</li> <li>At the top-right, click the New button to create a new integration.</li> <li>Ensure that the Enabled toggle is switched on to activate this integration.</li> <li>Fill in the Integration Name with a descriptive name (e.g., \"App Deployment Notification\").</li> <li>In the Post to Channel field, enter <code>#app-dev-team</code>.</li> <li>In the Post As field, enter your own username or another account name for clarity.</li> <li>Set the Alias to <code>app-dev-deployment</code> to identify the source of the notification.</li> <li>To help differentiate automated bot messages from user-generated messages, you can:</li> <li>Add an emoji (e.g., <code>:robot:</code>) in the Emoji field, which will replace the default avatar.</li> <li>Alternatively, use a custom avatar by inserting an image URL (e.g., <code>https://i.imgur.com/7U3Yv1e.png</code>).</li> <li>Once the fields are filled, click Save at the bottom-right to create the integration.</li> </ol>"},{"location":"rocketchat-notifications.html#2-update-the-repository-to-use-the-webhook","title":"2. Update the Repository to Use the Webhook","text":"<p>After setting up the webhook in RocketChat, you need to update your repository to use it:</p> <ol> <li>Navigate to the Settings tab of this repository on GitHub.</li> <li>Click on the Environments menu.</li> <li>Under the Environment Secrets section, update the <code>ROCKETCHAT_WEBHOOK_URL</code> with the new webhook URL from RocketChat.</li> </ol>"},{"location":"selecting-image-scanner.html","title":"Image Scanning for Vulnerabilities","text":""},{"location":"selecting-image-scanner.html#introduction","title":"Introduction","text":"<p>Vulnerability scanning tools are essential for identifying security risks in software applications and their underlying infrastructure. This summary examines seven prominent tools: Trivy, Clair, Anchore, Snyk, Grype, Aqua Security, and Qualys. These tools vary in their focus, coverage, and features, catering to different needs from simple open-source solutions to comprehensive enterprise-grade offerings. We can better select the tool(s) that aligns with our security requirements and expectations, by understanding their key differences.</p>"},{"location":"selecting-image-scanner.html#selection-criteria","title":"Selection Criteria","text":"<p>To comparatively evaluate the vulnerability scanning tools, the following selection criteria were considered:</p> <ol> <li>Vulnerability Coverage: The extent to which the tool identifies vulnerabilities across OS, application dependencies, and compliance requirements.</li> <li>Features: Additional capabilities, such as real-time monitoring, customizable policies, and secrets detection.</li> <li>Ease of Use: The user-friendliness of the tool and the complexity of its setup and configuration.</li> <li>Integration: Compatibility with CI/CD pipelines, Kubernetes, and other development tools.</li> <li>Cost: Pricing models, including free versions, basic features, and enterprise-grade options.</li> </ol>"},{"location":"selecting-image-scanner.html#comparative-analysis","title":"Comparative Analysis","text":"<p>This table below provides a clear comparison of the tools based the above selection criteria.</p> Tool Type Vulnerability Coverage Features Ease of Use Integration Cost Trivy Open-source High (OS + app dependencies) Simple, fast, secrets detection High CI/CD, Kubernetes Free Clair Open-source OS-level vulnerabilities Layered image scanning Moderate Quay, Kubernetes Free Anchore Open-source/Enterprise High (OS + app + compliance) Customizable policies, compliance Moderate Jenkins, CircleCI, K8s Free (basic), Paid Snyk Open-source/Commercial High (OS + app + IaC) Real-time monitoring, open-source focus High CI/CD, GitHub, Registries Free (basic), Paid Grype Open-source High (OS + app dependencies) Basic vulnerability scanning High CI/CD, Kubernetes Free Aqua Security Enterprise Comprehensive Runtime protection, compliance Moderate CI/CD, Kubernetes, Docker Paid (enterprise-grade) Qualys Enterprise Comprehensive Real-time, continuous monitoring Moderate CI/CD, Kubernetes, Docker Paid (enterprise-grade) <p>Given our team's emphasis on open-source solutions, we have streamlined our selection to Trivy, Clair, Anchore, Snyk, and Grype. We also prioritize ease of use, implementation speed, and execution speed as essential features in our technical decisions. Additionally, we strive to minimize costs wherever possible. Based on the table, Trivy meets all these selection criteria and will be used to implement image scanning in our GitHub workflows.</p>"},{"location":"team-conventions.html","title":"Team Conventions","text":""},{"location":"team-conventions.html#conventional-commits","title":"Conventional Commits","text":"<p>Conventional Commits provide a structured way to format commit messages. This improves readability, enables automated processes like changelog generation and versioning, and enhances collaboration through clearer communication in code repositories.</p> <ul> <li>Please refer to https://www.conventionalcommits.org/ for more detailed information.</li> </ul>"},{"location":"team-conventions.html#examples","title":"Examples","text":"<ul> <li><code>feat(user-auth): add password reset functionality</code></li> <li><code>fix(validation): handle edge case in email input</code></li> <li><code>chore(tests): update unit tests for user service</code></li> <li><code>refactor(api): optimize database queries in user endpoints</code></li> <li><code>docs(readme): update installation instructions</code></li> <li><code>chore(deps): update package versions for security patch</code></li> <li><code>BREAKING CHANGE: remove support for Node.js v10</code></li> </ul>"},{"location":"team-conventions.html#git-branching-model","title":"Git Branching Model","text":"<p>A Git branching model is a set of conventions that a development team follows when creating and managing branches in a Git repository. It provides a systematic approach to organizing code development, collaboration, and release management.</p>"},{"location":"team-conventions.html#github-flow","title":"GitHub flow","text":"<p>Considering the relatively small size of the project, this project adopts GitHub Flow. <code>GitHub Flow</code> is a lightweight, branch-based workflow designed around simplicity and continuous delivery. It's often used by teams working with Git and GitHub for version control and collaboration. <code>GitHub Flow</code> emphasizes frequent deployments to production and encourages a streamlined approach to development. Here are the key steps in <code>GitHub Flow</code>:</p> <ol> <li> <p>Create a Branch:    When starting work on a new feature, improvement, or bug fix, create a new branch. This branch will contain all the changes related to the task.</p> </li> <li> <p><code>feat/&lt;ticket#&gt;</code></p> </li> <li> <p><code>fix/&lt;ticket#&gt;</code></p> </li> <li> <p>Add Commits:    Make small, incremental commits to the branch as you work on the feature or fix. Each commit should represent a logical, standalone change.</p> </li> <li> <p>Open a Pull Request:    When you're ready to share your work, open a Pull Request (PR) on GitHub. Ensure all CI checks pass to request peer reviews.</p> </li> <li> <p>Discuss and Review:    Collaborators can review the changes, comment on specific lines of code, and discuss the implementation within the PR.</p> </li> <li> <p>Make Changes (if necessary):    Based on the feedback received during the review, make any necessary adjustments by adding more commits to the branch.</p> </li> <li> <p>Merge the PR:    Once the changes are approved and any requested modifications are made, the PR can be merged into the main branch.</p> </li> <li> <p>Deploy to Development:    After merging to the main branch, the changes are quickly integrated into the live development environment via an automated CD pipeline.</p> </li> <li> <p>Delete the Branch:    Once the changes are merged, the feature branch can be safely deleted unless there are more changes required to finish the features/fixes.</p> </li> </ol> <p><code>GitHub Flow</code> is often favored for its simplicity and speed. It encourages a continuous delivery approach, where changes are deployed <code>frequently and in smaller increments</code>, reducing the risk associated with large, infrequent releases. This workflow is well-suited for teams working on web applications and services where rapid iteration and deployment are crucial.</p>"},{"location":"team-conventions.html#considerations","title":"Considerations","text":"<ul> <li>Draft Pull Request:</li> </ul> <p>A <code>Draft Pull Request</code> in GitHub is a special type of pull request that indicates that the changes it contains are still a work in progress and not yet ready for review or merging. This feature is useful when you want to share your work with others for visibility or collaboration, but you're not seeking immediate feedback or approval.</p> <ul> <li> <p>Please refer to https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests#draft-pull-requests for more detailed information.</p> </li> <li> <p>Interactive Rebasing:</p> </li> </ul> <p><code>Interactive rebasing</code> allows you to have fine-grained control over your commit history, enabling you to clean up, reorder, and squash commits for a cleaner commit history, reducing noise in pull requests.</p> <p>For more information, visit:</p> <ul> <li>Git Rebase Documentation</li> <li> <p>Git Tools - Rewriting History</p> </li> <li> <p>Frequent Rebasing:</p> </li> </ul> <p><code>Frequent rebasing</code> is a vital practice in our workflow, ensuring your feature branch is based on the latest code. This <code>minimizes conflicts and eases integration</code>. To rebase, use <code>git pull origin main --rebase</code> in your feature branch. This promotes a smoother, collaborative development process.</p>"},{"location":"team-conventions.html#deployment-release-life-cycle","title":"Deployment &amp; Release Life Cycle","text":"<p>For this project, we manage three distinct environments: Development, Test, and Production. Each of these demands a tailored and efficient deployment process.</p>"},{"location":"team-conventions.html#development","title":"Development","text":"<p>The Development environment undergoes continuous deployment whenever changes are made to the main branch. This enables us to assess the current state of the application based on the main branch's codebase. Within the pipeline, a new container image is generated and tagged with the Git commit hash. It's then pushed to the GitHub container registry for use in the deployment process.</p> <ul> <li>Please refer to deploy-dev.yml for more detailed information.</li> </ul>"},{"location":"team-conventions.html#test","title":"Test","text":"<p>The Test environment experiences continuous deployment upon the <code>creation of a new Git tag</code>. Within the pipeline, a new container image is generated and tagged with the Git tag version. It is then pushed to the GitHub container registry for use in the deployment.</p> <ul> <li>Please refer to release-tag-changelog.yml for more detailed information.</li> </ul> <p>It's important to note that the workflow pipeline automatically <code>initiates the deployment of the test environment</code> concurrently with the <code>generation of a PR that incorporates the changes log</code>. Kindly review the PR and merge it into the main branch.</p>"},{"location":"team-conventions.html#production","title":"Production","text":"<p>Deployment to the Production environment is triggered upon the creation of a new GitHub Release. Unlike in Test, a new container image is not created. Instead, we leverage the existing container image that was generated for the Test environment. This approach provides the added benefit of using verified container images from the Test environment.</p>"},{"location":"team-conventions.html#ci-pipeline-checks","title":"CI Pipeline Checks","text":"<p>In this project, we employ two distinct CI checks to facilitate the seamless integration of new code before seeking peer reviews from colleagues.</p> <ol> <li> <p>Pre-commit Hook Checks    Pre-commit hook checks are configured within the local development environment based on the specifications outlined in .pre-commit-config.yaml. They serve to verify that the code changes in the developer's local environment meet the prescribed standards for code quality and security. To enforce these code quality standards across the entire repository, we enable the same set of checks in the CI pipeline.</p> </li> <li> <p>Please refer to test.yml for more detailed information.</p> </li> <li> <p>Application Build &amp; Testing    To ensure that no unbuildable or untestable code changes are merged into the development environment, we conduct a series of basic scenario tests. This process guarantees that the new changes exhibit no linting issues, build problems, and pass the defined unit tests present in the codebase.</p> </li> <li> <p>Please refer to test.yml for more detailed information.</p> </li> </ol>"},{"location":"team-conventions.html#peer-review-protocol","title":"Peer Review Protocol","text":"<p>To enhance clarity among contributors and facilitate effective peer reviews, we adhere to a standardized peer review process as a team. This process involves the following steps:</p> <ol> <li>Implement the entire or partial feature/fix on the designated branches.</li> <li>Ensure that the CI pipelines pass after pushing changes to the remote branch.</li> <li>Create a pull request to the main branch, ensuring there are no code conflicts.</li> <li>Notify team members of the new pull request in the team channel.</li> <li>Any available team member for peer review should acknowledge by reacting with an <code>eye emoji (\ud83d\udc40)</code> in the channel thread.</li> <li>The reviewer provides feedback on the pull request using the GitHub UI or initiates a discussion within the channel thread.</li> <li>If there are comments on the pull request, the reviewer may react to the thread with a <code>pencil emoji (\u270f\ufe0f)</code>.</li> <li>Once the reviewer is satisfied with the pull request, or if changes have been made following the initial review, they should react to the thread with a <code>thumbs-up emoji (\ud83d\udc4d)</code>.</li> <li>After the pull request is approved, the writer can merge the pull request and subsequently delete the branch unless there is no further changes.</li> </ol>"},{"location":"team-conventions.html#automated-dependency-updates","title":"Automated Dependency Updates","text":"<p>Automated dependency management plays a crucial role in keeping our project's libraries and dependencies up-to-date. This helps ensure that we benefit from the latest features, bug fixes, and security patches without manual intervention. We have two primary tools in place to facilitate this process:</p> <ul> <li> <p>Dependabot: Dependabot is a widely used automated dependency management tool that actively scans our project for outdated or vulnerable dependencies. It automatically opens pull requests with updated versions, allowing us to review and merge them with confidence.</p> </li> <li> <p>Renovate: Renovate is another powerful tool for automating dependency updates. It actively monitors our project's dependencies and creates pull requests to update them when new versions are available. Renovate also provides advanced customization options, making it a versatile choice for dependency management.</p> </li> <li>Please refer to Renovate Dashboard for more detailed information.</li> </ul>"},{"location":"team-conventions.html#secret-scanning","title":"Secret Scanning","text":"<p>In order to identify and manage potential secrets within your Git repository, a secret scanning task is executed as part of a pre-commit hook. This task utilizes a tool called detect-secrets. To create or update a baseline file that captures the potential secrets currently present in your repository, run:</p> <pre><code>detect-secrets scan --exclude-files '(sandbox/mock-users\\.json|pnpm-lock\\.yaml|.*/pnpm-lock\\.yaml)$' &gt; .secrets.baseline\n</code></pre>"},{"location":"team-conventions.html#file-naming-conventions","title":"File Naming Conventions","text":"<p>Within Next.js applications, adhering to proper file naming conventions is essential. Here are three widely adopted practices:</p> <ol> <li>Camel Case for File Names and Pascal Case for Component Names</li> <li><code>Kebab Case for File Names and Pascal Case for Component Names</code></li> <li>Pascal Case for Both File Names and Component Names</li> </ol> <p>Outside the Next.js application folder, <code>kebab case</code> is commonly utilized for naming <code>folders and files</code>, especially in URLs. This preference is driven by several reasons:</p> <ul> <li> <p><code>URL Friendliness</code>: Kebab case, with its use of hyphens, contributes to URLs that are more readable and user-friendly. This enhances the overall user experience and facilitates easier navigation.</p> </li> <li> <p><code>Consistency Across Platforms</code>: Kebab case is supported consistently across various platforms, making it a pragmatic choice for ensuring compatibility and avoiding issues related to case sensitivity.</p> </li> <li> <p><code>SEO Considerations</code>: Search engines often interpret hyphens in URLs as space, potentially improving search result readability. This can positively impact your website's search engine optimization (SEO).</p> </li> </ul> <p>Choosing the second option, which involves <code>Kebab Case for file names and Pascal Case for component names</code>, is beneficial in scenarios where a more standardized and conventional naming approach is preferred within the Next.js application itself. This can promote code consistency and make it easier for developers to collaborate and understand the codebase. Additionally, adhering to a specific convention within the application can simplify naming-related decisions during development.</p>"},{"location":"team-conventions.html#utilizing-sharable-functions","title":"Utilizing Sharable Functions","text":"<p>To enhance readability and ease of maintenance in our business logic, we leverage 3rd party packages, such as <code>lodash-es</code>, for common function usage.</p> <p>If third-party packages do not provide suitable utilities, we create our own functions and organize them in designated project locations. We've identified three distinct use cases for these common functions, each following agreed-upon guidelines:</p> <ul> <li> <p><code>Lower Level Functions</code>: Reserved for functions essential at the project's lower level, such as framework-level wrappers and helpers.</p> </li> <li> <p>These functions reside in the <code>/core</code> directory.</p> </li> <li> <p><code>Very Generic Functions</code>: Generic functions versatile enough for use in any project, not limited to the current one.</p> </li> <li> <p>These functions are stored in the <code>/utils</code> directory.</p> </li> <li> <p><code>Less Generic Functions</code>: Functions serving as helpers specifically within this project.</p> </li> <li>These functions are housed in the <code>/helpers</code> directory.</li> </ul> <p>As a team consensus, we aim to include <code>one or multiple common functions per file</code> and refrain from using <code>export default</code>.</p>"},{"location":"team-conventions.html#validation-schema-enum-naming-conventions","title":"Validation Schema &amp; Enum Naming Conventions","text":"<p>The following naming conventions are used for validation schemas and enums with the validation tool Zod:</p> <ol> <li> <p>Validation Schema:</p> </li> <li> <p>Naming: Schema objects are named in camelCase and end with <code>Schema</code>.</p> </li> <li>Type: The TypeScript type derived from the schema object is in PascalCase.</li> </ol> <pre><code>const userSchema = z.object({\n  name: z.string().min(1, 'Name is required'),\n  age: z.number().min(0, 'Age must be a non-negative number'),\n});\n\ntype User = z.infer&lt;typeof userSchema&gt;;\n</code></pre> <ol> <li> <p>Enum:</p> </li> <li> <p>Naming: Enum objects are named in PascalCase and end with <code>Enum</code>.</p> </li> <li>Type: The TypeScript type derived from the enum object is in PascalCase.</li> </ol> <pre><code>const ColorEnum = z.enum(['Red', 'Green', 'Blue']);\n\ntype Color = z.infer&lt;typeof ColorEnum&gt;;\n</code></pre>"},{"location":"team-conventions.html#explanation","title":"Explanation:","text":"<ul> <li> <p>Validation Schema Naming:</p> </li> <li> <p><code>userSchema</code>: The schema object for user data, named in camelCase and ending with <code>Schema</code>.</p> </li> <li> <p><code>User</code>: The TypeScript type inferred from <code>userSchema</code>, named in PascalCase.</p> </li> <li> <p>Enum Naming:</p> </li> <li><code>ColorEnum</code>: The enum object for color options, named in PascalCase and ending with <code>Enum</code>.</li> <li><code>Color</code>: The TypeScript type inferred from <code>ColorEnum</code>, named in PascalCase.</li> </ul>"},{"location":"terraform.html","title":"Terraform Configuration for Infrastructure Provisioning","text":"<p>We employ <code>Terraform</code> as the <code>configuration code</code> to <code>provision resources</code> within our infrastructure, encompassing <code>OCP Deployer Service Accounts</code>, <code>Keycloak</code>, and <code>Sysdig</code>.</p>"},{"location":"terraform.html#sysdig","title":"Sysdig","text":"<p>The main point for the <code>Sysdig Terraform</code> is situated at terraform/sysdig, featuring individual sub-modules for each Openshift project.</p>"},{"location":"terraform.html#local-machine-modifications","title":"Local Machine Modifications","text":"<p>For making changes on a local machine, please consult the Sysdig Terraform Readme for comprehensive instructions.</p>"},{"location":"terraform.html#cd-pipeline-driven-modifications","title":"CD Pipeline-driven Modifications","text":"<p>The Sysdig Terraform scripts are integrated into the <code>Continuous Deployment (CD) pipeline</code>, specifically the Sysdig Terraform Action. The process involves two distinct phases:</p> <ol> <li> <p>PR creation    Upon the <code>creation of a pull request</code> that includes changes to the Sysdig Terraform script, the pipeline executes a <code>Terraform Plan</code> and comments on the PR regarding the proposed alterations.</p> </li> <li> <p>PR merging    Upon approval and <code>merging of the pull request</code>, the pipeline executes a <code>Terraform Apply</code>, implementing the approved changes into the infrastructure.</p> </li> </ol>"},{"location":"clean-codes/return-early.html","title":"Return Early Pattern","text":"<p>The <code>Return Early</code> pattern for functions is a programming practice where you structure your code in such a way that you check for certain conditions at the beginning of a function and return immediately if those conditions are met. This pattern is often used to improve code readability, reduce nesting, and make the code more concise.</p>"},{"location":"clean-codes/return-early.html#benefits","title":"Benefits","text":"<p>Here are some benefits of using the <code>Return Early</code> pattern:</p> <ol> <li> <p><code>Improved Readability</code>: By checking for special cases or error conditions at the beginning of a function and returning early, you make the code easier to read and understand. Developers can quickly grasp the main flow of the function without getting bogged down in nested if statements.</p> </li> <li> <p><code>Reduced Nesting</code>: Avoiding deep nesting of if statements or other control structures makes the code more readable and less prone to errors. Each early return eliminates a level of indentation, making the code flatter and easier to follow.</p> </li> <li> <p><code>Faster Execution</code>: In some cases, the <code>Return Early</code> pattern can lead to faster execution. If the function encounters a condition that allows it to exit early, it doesn't need to execute the rest of the code. This can be beneficial for performance, especially in situations where the conditions for early return are frequently met.</p> </li> <li> <p><code>Easier Maintenance</code>: Code that follows the <code>Return Early</code> pattern is often easier to maintain. When you need to make changes or add new features, you can focus on specific sections of the function without having to understand the entire flow. This can lead to more modular and maintainable code.</p> </li> <li> <p><code>Clearer Intent</code>: The pattern helps express the intent of the code more clearly. When you check for special cases first and return early, it highlights the primary path of the function. This can be helpful for anyone reading the code, including the original developer and others who may need to maintain or debug the code later.</p> </li> </ol>"},{"location":"clean-codes/return-early.html#examples","title":"Examples","text":""},{"location":"clean-codes/return-early.html#good-example-using-return-early-pattern","title":"Good Example (Using \"Return Early\" Pattern)","text":"<pre><code>function calculateTotal(price, quantity) {\n  // Check for invalid inputs\n  if (price &lt;= 0 || quantity &lt;= 0) {\n    return 0; // Return early if inputs are invalid\n  }\n\n  // Main calculation\n  let total = price * quantity;\n\n  // Additional calculations or logic\n\n  return total;\n}\n</code></pre>"},{"location":"clean-codes/return-early.html#bad-example-without-return-early-pattern","title":"Bad Example (Without \"Return Early\" Pattern)","text":"<pre><code>function calculateTotal(price, quantity) {\n  // No \"Return Early\" pattern, using nested if statements\n\n  // Check for invalid inputs\n  if (price &gt; 0) {\n    if (quantity &gt; 0) {\n      // Main calculation\n      let total = price * quantity;\n\n      // Additional calculations or logic\n\n      return total;\n    } else {\n      return 0; // Return if quantity is invalid\n    }\n  } else {\n    return 0; // Return if price is invalid\n  }\n}\n</code></pre> <p>In the bad example, the absence of the <code>Return Early</code> pattern leads to nested if statements. This can make the code harder to read and understand, especially as the complexity of the function increases. The good example using the <code>Return Early</code> pattern is more concise, readable, and easier to maintain. It avoids unnecessary nesting and clearly expresses the main flow of the function.</p>"},{"location":"clean-codes/return-early.html#considerations-for-complex-control-flow","title":"Considerations for Complex Control Flow","text":"<p>In scenarios where a function exhibits highly complex control flow with multiple conditions and branches, it might be advisable to exercise caution in employing the <code>Early Return</code> pattern. Excessive use in such cases could lead to fragmented and less readable code. Striking a balance by maintaining a structured flow with fewer early returns may be more appropriate in such intricate contexts.</p>"},{"location":"poc/security-dashboard.html","title":"Security Dashboard Research","text":""},{"location":"poc/security-dashboard.html#requirements","title":"Requirements","text":"<p>Our primary goal is to enhance the security monitoring of applications deployed on Openshift. To achieve this, we aim to implement both Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST) processes. Key requirements include:</p> <ul> <li><code>SAST and DAST Implementation</code>: Utilize tools such as SonarQube for SAST and ZAP (Zed Attack Proxy) for DAST to assess the security status of applications.</li> <li><code>Support for Private Repositories</code>: Ensure that the security scanning processes extend support to projects hosted in private repositories.</li> </ul>"},{"location":"poc/security-dashboard.html#proposals","title":"Proposals","text":""},{"location":"poc/security-dashboard.html#sonarqube-deployment","title":"SonarQube Deployment","text":"<p>In order to efficiently run and collect security scan results, we propose deploying a dedicated instance of <code>SonarQube</code>. This deployment will serve as a centralized platform for managing and analyzing the security posture of the applications.</p>"},{"location":"poc/security-dashboard.html#scheduler-runner-deployment","title":"Scheduler Runner Deployment","text":"<p>To facilitate the automation of security scans, we propose the deployment of a custom scheduler runner. This runner should have sufficient privileges to access both public and private repositories, as well as the Openshift cluster's Route objects.</p> <p>The scheduler runner will conduct security scans on both the repository codebases and live URLs identified in Openshift Route objects. Following the completion of the scans, the runner will transmit the results to the Platform Service application.</p>"},{"location":"poc/security-dashboard.html#platform-service-application","title":"Platform Service Application","text":"<p>To enhance the overall functionality of the security dashboard, we suggest implementing the following updates to the Platform Service application:</p> <ul> <li> <p><code>UI Enhancement</code>: Revise the user interface to incorporate a feature allowing users to input <code>repository URLs</code>, thereby enabling Static Application Security Testing (SAST) through SonarQube.</p> </li> <li> <p><code>Result Storage Endpoint</code>: Introduce a new endpoint in the application to receive and store the scanning results generated by the <code>scheduler runner</code>. This endpoint will play a crucial role in maintaining a comprehensive database of security information.</p> </li> </ul> <p>By implementing these proposals, we aim to establish a robust and automated security monitoring system that not only supports the latest security testing methodologies but also provides a seamless and user-friendly experience for managing and analyzing security scan results.</p>"}]}